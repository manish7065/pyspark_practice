{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOrhIS39r+b8bcv5jd87hTQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manish7065/pyspark_practice/blob/master/sql_complex_querry.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJOmkNAq14b5"
      },
      "outputs": [],
      "source": [
        "print(\"hi\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: write a code to read tab seperate csv file using pyspark to create data frame\n",
        "\n",
        "!pip install pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"ReadTabCSV\").getOrCreate()\n",
        "\n",
        "df = spark.read.csv(\"path/to/your/file.csv\", sep=\"\\t\", header=True, inferSchema=True)\n",
        "\n",
        "# Show the DataFrame\n",
        "df.show()\n"
      ],
      "metadata": {
        "id": "qqrnwajG2d00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kkQo9eZWBTGq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "employee_id name    experience_years\n",
        "1   raam\t3\n",
        "2\tjohn\t2\n",
        "3\tjoe\t\t3\n",
        "4\tthomas\t2\n"
      ],
      "metadata": {
        "id": "aBVRC3l_BQ6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "id": "v_4Osbb1B1VE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: write a code to create df from above data as csv\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "print(\"hi\")\n",
        "\n",
        "# !pip install pyspark\n",
        "\n",
        "spark = SparkSession.builder.appName(\"ReadTabCSV\").getOrCreate()\n",
        "\n",
        "data = [(1, \"raam\", 3), (2, \"john\", 2), (3, \"joe\", 3), (4, \"thomas\", 2)]\n",
        "columns = [\"employee_id\", \"name\", \"experience_years\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Show the DataFrame\n",
        "df.show()\n",
        "\n",
        "# Write the DataFrame to a CSV file\n",
        "df.write.csv(\"output.csv\", header=True, mode=\"overwrite\")\n"
      ],
      "metadata": {
        "id": "3kMD8oX7Bd6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: similarly create the df using\n",
        "# Project_id  employee_id\n",
        "# 1   1\n",
        "# 1\t2\n",
        "# 1\t3\n",
        "# 2\t1\n",
        "# 2\t4\n",
        "\n",
        "data = [(1, 1), (1, 2), (1, 3), (2, 1), (2, 4)]\n",
        "columns = [\"Project_id\", \"employee_id\"]\n",
        "proj_df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Show the DataFrame\n",
        "proj_df.show()\n"
      ],
      "metadata": {
        "id": "FQe4pYMUBtfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.createOrReplaceTempView(\"employee\")"
      ],
      "metadata": {
        "id": "1iikE7LqDLRr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "proj_df.createOrReplaceTempView(\"project\")"
      ],
      "metadata": {
        "id": "eHrKtmZADigb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "querry = \"\"\"\n",
        "SELECT e.name, p.Project_id\n",
        "FROM employee e p\n",
        "JOIN project p ON employee.employee_id = project.employee_id\n",
        "group by  p.Project_id\n",
        "having max(e.experience_years)\n",
        "\"\"\"\n",
        "\n",
        "spark.sql(querry).show()"
      ],
      "metadata": {
        "id": "Sr_MQjKYDlSb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: write a code to fetch the max experienced employee project wise\n",
        "\n",
        "df.createOrReplaceTempView(\"employee\")\n",
        "proj_df.createOrReplaceTempView(\"project\")\n",
        "query = \"\"\"\n",
        "SELECT e.name, p.Project_id\n",
        "FROM employee e\n",
        "JOIN project p ON e.employee_id = p.employee_id\n",
        "WHERE (p.Project_id, e.experience_years) IN (\n",
        "    SELECT Project_id, MAX(experience_years)\n",
        "    FROM employee e\n",
        "    JOIN project p ON e.employee_id = p.employee_id\n",
        "    GROUP BY Project_id\n",
        ")\n",
        "\"\"\"\n",
        "\n",
        "spark.sql(query).show()\n"
      ],
      "metadata": {
        "id": "rp1_VxA2EEjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: why its giving 3 output there should have to be only one max experienced employee project wise\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "print(\"hi\")\n",
        "\n",
        "!pip install pyspark\n",
        "\n",
        "spark = SparkSession.builder.appName(\"ReadTabCSV\").getOrCreate()\n",
        "\n",
        "data = [(1, \"raam\", 3), (2, \"john\", 2), (3, \"joe\", 3), (4, \"thomas\", 2)]\n",
        "columns = [\"employee_id\", \"name\", \"experience_years\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Show the DataFrame\n",
        "df.show()\n",
        "\n",
        "# Write the DataFrame to a CSV file\n",
        "df.write.csv(\"output.csv\", header=True, mode=\"overwrite\")\n",
        "\n",
        "\n",
        "data = [(1, 1), (1, 2), (1, 3), (2, 1), (2, 4)]\n",
        "columns = [\"Project_id\", \"employee_id\"]\n",
        "proj_df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Show the DataFrame\n",
        "proj_df.show()\n",
        "\n",
        "df.createOrReplaceTempView(\"employee\")\n",
        "proj_df.createOrReplaceTempView(\"project\")\n",
        "\n",
        "# Modified Query to get only one max experienced employee project wise\n",
        "query = \"\"\"\n",
        "SELECT e.name, p.Project_id\n",
        "FROM employee e\n",
        "JOIN project p ON e.employee_id = p.employee_id\n",
        "WHERE (p.Project_id, e.experience_years) IN (\n",
        "    SELECT Project_id, MAX(experience_years)\n",
        "    FROM employee e\n",
        "    JOIN project p ON e.employee_id = p.employee_id\n",
        "    GROUP BY Project_id\n",
        ")\n",
        "\"\"\"\n",
        "\n",
        "result_df = spark.sql(query)\n",
        "\n",
        "# To get only one row per project, we can use distinct()\n",
        "distinct_result_df = result_df.distinct()\n",
        "\n",
        "distinct_result_df.show()\n"
      ],
      "metadata": {
        "id": "840DaXemEiH3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: write a code to fetch the max experienced employee project wise\n",
        "\n",
        "df.createOrReplaceTempView(\"employee\")\n",
        "proj_df.createOrReplaceTempView(\"project\")\n",
        "query = \"\"\"\n",
        "SELECT p.project_id, e.name\n",
        "FROM project p\n",
        "JOIN employee e ON e.employee_id = p.employee_id\n",
        "WHERE (p.project_id, e.experience_years) IN (\n",
        "    SELECT p2.project_id, MAX(e2.experience_years)\n",
        "    FROM project p2\n",
        "    JOIN employee e2 ON e2.employee_id = p2.employee_id\n",
        "    GROUP BY p2.project_id\n",
        ");\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "spark.sql(query).show()\n"
      ],
      "metadata": {
        "id": "29OtBkXlFMI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "querr = \"\"\"\n",
        "select e.name, p.project_id from  employee e\n",
        "join project p on p.employee_id = e.employee_id\n",
        "where (p.project_id, e.experience_years) in (\n",
        "    select\n",
        "        p2.project_id,\n",
        "        max(e2.experience_years)\n",
        "    from project p2\n",
        "    join employee e2 on e2.employee_id = p2.employee_id\n",
        "    group by p2.project_id\n",
        ")\n",
        "\"\"\"\n",
        "\n",
        "spark.sql(querr).show()"
      ],
      "metadata": {
        "id": "dfUT_GeEFmHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"\"\"\n",
        "SELECT\n",
        "    p.project_id,\n",
        "    e.employee_id,\n",
        "    e.name,\n",
        "    e.experience_years\n",
        "FROM\n",
        "    project p\n",
        "JOIN\n",
        "    employee e ON p.employee_id = e.employee_id\n",
        "WHERE\n",
        "    (p.project_id, e.experience_years) IN (\n",
        "        SELECT\n",
        "            project_id,\n",
        "            MAX(e2.experience_years)\n",
        "        FROM\n",
        "            project p2\n",
        "        JOIN\n",
        "            employee e2 ON p2.employee_id = e2.employee_id\n",
        "        GROUP BY\n",
        "            p2.project_id\n",
        "    );\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "spark.sql(query).show()"
      ],
      "metadata": {
        "id": "6-LVJ4-_H9Yp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2T8oFCHmJWoC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}