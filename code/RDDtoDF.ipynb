{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDD to DF incorporating on top of rdds\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('RDD to DF').master('Local[*]').getOrCreate()\n",
    "\n",
    "employee = [('manish',20000),('suresh',23000),('mahesh',430000)]\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(employee)\n",
    "\n",
    "for emp in rdd.collect():\n",
    "    print(emp)\n",
    "\n",
    "\n",
    "df = rdd.toDF()\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('RDD to DF').master('Local[*]').getOrCreate()\n",
    "\n",
    "employee = [('manish',20000),('suresh',23000),('mahesh',430000)]\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(employee)\n",
    "\n",
    "empdf = spark.createDataFrame(rdd,schema=['name','salary'])\n",
    "empdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By Defining Schema\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('RDD to DF').master('Local[*]').getOrCreate()\n",
    "\n",
    "employee = [('manish',20000),('suresh',23000),('mahesh',430000)]\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(employee)\n",
    "\n",
    "empdf = spark.createDataFrame(rdd,schema=['name','salary'])\n",
    "empdf.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
